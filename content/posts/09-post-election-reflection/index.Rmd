---
title: "09-Post-Election Reflection"
author: "Kento Yamada"
date: '2022-11-22'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_knit$set(root.dir = '/Users/kentoyamada/Documents/R-Projects/election-analytics-2022-midterms')
```

```{r load packages, include = FALSE}
library(tidyverse)
library(readxl)
library(sf)
library(stringi)
library(kableExtra)
library(stargazer)
library(usdata)
library(cdlTools)
library(gt)
```

```{r load data, include = FALSE}
# Data on GDP/quarter
gdp <- read_csv("analysis_data/GDP_quarterly.csv") %>%
  select(-c("...1", "...2")) # de-select unnecessary columns

# Data on vote/seat share
share <- read_csv("analysis_data/house_popvote_seats.csv") %>%
  select(-c("...1", "AreaAll")) # de-select unnecessary columns

# Generic ballot
generic <- read_csv("analysis_data/GenericPolls1942_2020_updated.csv")
# 2022 generic ballot
recent_generic <- read_csv("analysis_data/1105_538_generic_ballot_polls.csv")

# Expert ratings
ratings <- read_csv("analysis_data/expert_rating.csv")
# Expert ratings data for 2022
ratings_2022 <- read_excel("analysis_data/expert_ratings_2022.xlsx")
# Latest expert ratings data
ratings_2022_10_27 <- read_csv("analysis_data/expert_rating_2022-10-27.csv")

# Vote share by district
vote_share <- read_csv("analysis_data/house_party_vote_share_by_district_1948-2020.csv")

# Vote share in Presidential elections
pres_vote_share_2016 <- read_csv("analysis_data/kos_2016_pres_2018_districts.csv", skip = 1)
pres_vote_share_2012 <- read_csv("analysis_data/kos_2012_pres_2014_districts.csv")
pres_vote_share_2020 <-  read_csv("analysis_data/dra_2020_pres.csv") %>%
  select(-"...1")

# Incumbency
incumb <- read_csv("analysis_data/incumb_dist_1948-2020.csv")
incumb_2022 <- read_csv("analysis_data/house_cands.csv")

# District-level polls
district_polls <- read_csv("analysis_data/house_polls_long.csv")
```


```{r clean gdp_df, include = FALSE}
# Select relevant columns
gdp_df_relevant <- gdp %>%
  select(year, quarter_cycle, GDPC1)

# Filter out data for Q1
gdp_df_q1 <- gdp_df_relevant %>%
  filter(quarter_cycle == 1) %>%
  # Note that election years are one year after Q1
  mutate(election_year = year + 1) %>% 
  rename(gdp_q1 = GDPC1) %>%
  select(-c(quarter_cycle, year))

# Filter out data for Q6
gdp_df_q6 <- gdp_df_relevant %>%
  filter(quarter_cycle == 6) %>%
  select(-quarter_cycle) %>%
  rename(gdp_q6 = GDPC1, election_year = year)

# Filter out data for Q7
gdp_df_q7 <- gdp_df_relevant %>%
  filter(quarter_cycle == 7) %>%
  select(-quarter_cycle) %>%
  rename(gdp_q7 = GDPC1, election_year = year)

# Merge and finalize data
gdp_df_final <- left_join(gdp_df_q7, gdp_df_q6, by = "election_year") %>%
  left_join(gdp_df_q1, by = "election_year") %>%
  mutate(gdp_q7_q6 = (gdp_q7 - gdp_q6) / gdp_q6 * 100,
         gdp_q7_q1 = (gdp_q7 - gdp_q1) / gdp_q1 * 100) %>%
  select(-c(gdp_q7, gdp_q6, gdp_q1))
```

```{r clean share_df, include = FALSE}
# Define incumbent president's party affiliation
share_df <- share
# In a Presidential election year, share_df$president_party shows the party 
# of the candidate who won the Presidential election.
# Thus, in cases where the party that the incumbent president belongs to
# loses the Presidential election,
# share_df$president_party is different from the incumbent president's party
share_df$inc_pres_party <- share_df$president_party
share_df$inc_pres_party[which(share_df$year == 1952)] <- "D"
share_df$inc_pres_party[which(share_df$year == 1960)] <- "R"
share_df$inc_pres_party[which(share_df$year == 1968)] <- "D"
share_df$inc_pres_party[which(share_df$year == 1976)] <- "R"
share_df$inc_pres_party[which(share_df$year == 1980)] <- "D"
share_df$inc_pres_party[which(share_df$year == 1992)] <- "R"
share_df$inc_pres_party[which(share_df$year == 2000)] <- "D"
share_df$inc_pres_party[which(share_df$year == 2008)] <- "R"
share_df$inc_pres_party[which(share_df$year == 2016)] <- "D"
share_df$inc_pres_party[which(share_df$year == 2020)] <- "R"

share_df_final <- share_df %>%
  mutate(pp_vote_share = # Two-party vote share of the incumbent President's party
           case_when(inc_pres_party == "D" ~ D_majorvote_pct,
                     inc_pres_party == "R" ~ R_majorvote_pct),
         midterm = # Midterm election or not
           case_when(year %% 4 == 0 ~ FALSE,
                     year %% 4 == 2 ~ TRUE)) %>%
  select(year, pp_vote_share, midterm, 
         inc_pres_party, president_party) %>%
  rename(election_year = year)
```

```{r clean generic_df, include = FALSE }
generic_df <- generic %>%
  # Filter out election year and discard polls conducted after election
  filter(year %% 2 == 0) %>%
  filter(days_until_election > 0, days_until_election < 365) %>%
  select(year, emonth, eday, dem, rep, days_until_election) %>%
  rename(dem_poll = dem, rep_poll = rep) %>%
  # calculate poll lead
  mutate(dem_lead = dem_poll - rep_poll) %>%
  # add weights
  mutate(recency = 365 - days_until_election)
```

```{r generic_df_df_final, include = FALSE}
generic_df_final <- generic_df %>%
  # calculate weighted average
  group_by(year) %>%
  # Add weights
  summarize(dem_lead_weighted = weighted.mean(dem_lead, recency)) %>%
  rename(election_year = year)
```

```{r finalize df_national, include = FALSE}
df_national <- full_join(gdp_df_final, share_df_final, by = "election_year") %>%
  full_join(generic_df_final, by = "election_year")

# Calculate polling data for President's party
df_national <- df_national %>%
  # Lead in the generic ballot poll of the incumbent President's party
  mutate(pp_generic_lead= 
           case_when(inc_pres_party == "D" ~ dem_lead_weighted,
                     inc_pres_party == "R" ~ -dem_lead_weighted))
```

```{r model_national_1, include = FALSE}
model_national_1 <- lm(pp_vote_share ~ midterm + pp_generic_lead, 
             data = df_national)
```

```{r rmse_model_national_1, include = FALSE}
# RMSE: sqrt(mean(resid(model_national_1)^2))
rmse_model_national_1 <- 
  (model_national_1$model$pp_vote_share - 
                            model_national_1$fitted.values)^2 %>%
  mean() %>% 
  sqrt()
```

```{r clean 2022 generic ballot data, include = FALSE}
recent_generic_df <- recent_generic %>%
  select(end_date, dem, rep) %>%
  mutate(end_date = as.Date(end_date, "%m/%d/%y")) %>%
  mutate(days_until_election = round(difftime("2022-11-8", end_date, units = "days"))) %>%
  # Filter out 2022
  filter(end_date >= "2022-01-01") %>%
  mutate(dem_lead = dem - rep) %>%
  # add weights
  mutate(recency = 365 - as.numeric(days_until_election))
```

```{r recent_generic_df, include = FALSE}
recent_generic_final <- recent_generic_df %>%
  # calculate weighted average
  summarize(pp_generic_lead = weighted.mean(dem_lead, recency))
```

```{r predictions, include = FALSE}
# Get data for 2022
latest_data <- 
  data.frame(midterm = TRUE, 
             pp_generic_lead = recent_generic_final$pp_generic_lead)
```

```{r predict popular vote share, include = FALSE}
# Prediction
model_national_1_pred <- predict.lm(model_national_1, latest_data, interval = "prediction")

# simulations
set.seed(12345)
nsims = 5000
model_national_1_pred_w_error <- 
  rnorm(nsims, mean = model_national_1_pred[1], sd = rmse_model_national_1)

# save predictions
# write.csv(as_tibble(model_national_1_pred_w_error), "national_dem_share_pred.csv")
```

```{r Democratic popular vote share 2012-2020, include = FALSE}
pop_share_df <- share_df %>%
  filter(year >= 2012, year <= 2020) %>%
  select(year, D_majorvote_pct) %>%
  rename(d_pop_share = D_majorvote_pct)
```

```{r clinton vote share 2016, echo = FALSE}
clinton_df <- pres_vote_share_2016 %>%
  select(CD, Clinton, Trump) %>%
  # Convert to two-party vote share
  mutate(Clinton_two_party = 100*Clinton/(Clinton + Trump)) %>%
  rename(District = CD, prev_pres_d = Clinton_two_party)

# Change district names
clinton_df$District[which(clinton_df$District == "AK-AL")] <- "AK-01"
clinton_df$District[which(clinton_df$District == "DE-AL")] <- "DE-01"
clinton_df$District[which(clinton_df$District == "MT-AL")] <- "MT-01"
clinton_df$District[which(clinton_df$District == "ND-AL")] <- "ND-01"
clinton_df$District[which(clinton_df$District == "SD-AL")] <- "SD-01"
clinton_df$District[which(clinton_df$District == "VT-AL")] <- "VT-01"
clinton_df$District[which(clinton_df$District == "WY-AL")] <- "WY-01"

# Define year
clinton_df$year <- 2018
```

```{r obama vote share 2012, echo = FALSE}
obama_df <- pres_vote_share_2012 %>%
  select(CD, `Obama 2012`, `Romney 2012`) %>%
  # Convert to two-party vote share
  mutate(Obama_two_party = 100*`Obama 2012`/(`Obama 2012` + `Romney 2012`)) %>%
  rename(District = CD, prev_pres_d = Obama_two_party)

# Change district names
obama_df$District[which(obama_df$District == "AK-AL")] <- "AK-01"
obama_df$District[which(obama_df$District == "DE-AL")] <- "DE-01"
obama_df$District[which(obama_df$District == "MT-AL")] <- "MT-01"
obama_df$District[which(obama_df$District == "ND-AL")] <- "ND-01"
obama_df$District[which(obama_df$District == "SD-AL")] <- "SD-01"
obama_df$District[which(obama_df$District == "VT-AL")] <- "VT-01"
obama_df$District[which(obama_df$District == "WY-AL")] <- "WY-01"

# Define year
obama_df$year <- 2014
```

```{r clean incumbency data, include = FALSE}
# Filter out 2012-2020
incumb_df <-  filter(incumb, year >= 2012 & year <= 2020)

# Find duplicates: summary(as.factor(incumb_df$district_id))
# Remove other duplicates caused by runoffs etc.
drop_2 <- c(
  which(incumb_df$district_id == "LA05" & incumb_df$year == 2020)[2],
  which(incumb_df$district_id == "LA03" & incumb_df$year == 2016)[2],
  which(incumb_df$district_id == "LA03" & incumb_df$year == 2012)[2],
  which(incumb_df$district_id == "LA04" & incumb_df$year == 2016)[1],
  which(incumb_df$district_id == "LA05" & incumb_df$year == 2014)[2],
  which(incumb_df$district_id == "ME02" & incumb_df$year == 2018)[2]
)
incumb_df <- incumb_df[-drop_2,]

# Code incumbent = 1
incumb_df <- incumb_df %>%
  mutate(d_inc = case_when(DemStatus == "Incumbent" ~ 1, 
                           TRUE ~ 0)) %>%
  rename(District = district_id) %>%
  select(year, District, d_inc)
  
# Format district names
stri_sub(incumb_df$District, 3, 2) <- "-"

# Change district names
incumb_df$District[which(incumb_df$District == "AK-00")] <- "AK-01"
incumb_df$District[which(incumb_df$District == "DE-00")] <- "DE-01"
incumb_df$District[which(incumb_df$District == "MT-00")] <- "MT-01"
incumb_df$District[which(incumb_df$District == "ND-00")] <- "ND-01"
incumb_df$District[which(incumb_df$District == "SD-00")] <- "SD-01"
incumb_df$District[which(incumb_df$District == "VT-00")] <- "VT-01"
incumb_df$District[which(incumb_df$District == "WY-00")] <- "WY-01"
```

```{r vote share data, include = FALSE}
vote_share_df <- vote_share %>%
  # Filter out data from 2012 to 2020
  filter(raceYear >= 2012, raceYear <= 2020) %>%
  rename(pct_D = DemVotesMajorPercent, District = CD, year = raceYear) # Rename columns

# Find duplicates: summary(as.factor(vote_share_df$district_id))
# Remove other duplicates caused by runoffs etc.
drop_3 <- c(
  which(vote_share_df$district_id == "LA03" & vote_share_df$year == 2016)[2],
  which(vote_share_df$district_id == "LA03" & vote_share_df$year == 2012)[2],
  which(vote_share_df$district_id == "LA05" & vote_share_df$year == 2020)[2],
  which(vote_share_df$district_id == "LA05" & vote_share_df$year == 2014)[2],
  which(vote_share_df$district_id == "LA04" & vote_share_df$year == 2016)[1],
  which(vote_share_df$district_id == "ME02" & vote_share_df$year == 2018)[2]
)
vote_share_df <- vote_share_df[-drop_3,]

# Select relevant columns
vote_share_df <- vote_share_df %>% 
  select(year, District, pct_D)

# Change district names
vote_share_df$District[which(vote_share_df$District == "AK-AL")] <- "AK-01"
vote_share_df$District[which(vote_share_df$District == "DE-AL")] <- "DE-01"
vote_share_df$District[which(vote_share_df$District == "MT-AL")] <- "MT-01"
vote_share_df$District[which(vote_share_df$District == "ND-AL")] <- "ND-01"
vote_share_df$District[which(vote_share_df$District == "SD-AL")] <- "SD-01"
vote_share_df$District[which(vote_share_df$District == "VT-AL")] <- "VT-01"
vote_share_df$District[which(vote_share_df$District == "WY-AL")] <- "WY-01"
```

```{r clean expert ratings data, include = FALSE}
ratings_df <- ratings %>%
  # Convert state name to state abbreviation
  mutate(State = state2abbr(state)) %>%
  # District numbers
  mutate(District = str_pad(district, 2, pad = "0")) %>%
  # Define district names
  mutate(District = paste(State, District, sep = "-")) %>%
  
  # Filter out data from 2012 to 2020
  filter(year >= 2012, year <= 2020) %>%
  
  select(District, year, cook, rothenberg, sabatos_crystal_ball)

# Calculate ratings average
ratings_df$rating_avg = rowMeans(ratings_df[, -c(1,2)])

# Change district names
ratings_df$District[which(ratings_df$District == "AK-AL")] <- "AK-01"
ratings_df$District[which(ratings_df$District == "MT-AL")] <- "MT-01"
ratings_df$District[which(ratings_df$District == "ND-AL")] <- "ND-01"
ratings_df$District[which(ratings_df$District == "SD-AL")] <- "SD-01"
```

```{r outcome by CD, include = FALSE}
d_win_df <- vote_share %>%
  # Filter out data from 2012 to 2020
  filter(raceYear >= 2012, raceYear <= 2020) %>%
  rename(year = raceYear, District = CD) %>% # Rename columns
  select(State, district_num, year, WinnerParty, District) %>%
  # 1 if Democrat won
  mutate(d_win = if_else(WinnerParty == "D", 1, 0))

# Find duplicates: summary(as.factor(d_win_df$District))
# Remove other duplicates caused by runoffs etc.
drop <- c(
  which(d_win_df$District == "LA-05" & d_win_df$year == 2020)[2],
  which(d_win_df$District == "LA-03" & d_win_df$year == 2016)[2],
  which(d_win_df$District == "LA-03" & d_win_df$year == 2012)[2],
  which(d_win_df$District == "LA-04" & d_win_df$year == 2016 & 
          d_win_df$WinnerParty == "D"),
  which(d_win_df$District == "LA-05" & d_win_df$year == 2014 & 
          d_win_df$WinnerParty == "D"),
  which(d_win_df$District == "ME-02" & d_win_df$year == 2018 & 
          d_win_df$WinnerParty == "R") # Ranked choice voting in ME
)
d_win_df <- d_win_df[-drop,]

# Change district names
d_win_df$District[which(d_win_df$District == "AK-AL")] <- "AK-01"
d_win_df$District[which(d_win_df$District == "DE-AL")] <- "DE-01"
d_win_df$District[which(d_win_df$District == "MT-AL")] <- "MT-01"
d_win_df$District[which(d_win_df$District == "ND-AL")] <- "ND-01"
d_win_df$District[which(d_win_df$District == "SD-AL")] <- "SD-01"
d_win_df$District[which(d_win_df$District == "VT-AL")] <- "VT-01"
d_win_df$District[which(d_win_df$District == "WY-AL")] <- "WY-01"

# Select relevant columns
d_win_df <- select(d_win_df, -c(State, district_num, WinnerParty))
```

```{r further clean expert ratings data, include = FALSE}
# First merge d_win_df and ratings_df
ratings_adj_df <- left_join(d_win_df, ratings_df, by = c("District", "year"))

# If rating was missing for a particular district, that means that the race was not close
# Assume that the rating was 7 if the Republicans ended up winning and 1 if the Democrats ended up winning
ratings_adj_df[is.na(ratings_adj_df$rating_avg),]$rating_avg <- 
  if_else(ratings_adj_df[is.na(ratings_adj_df$rating_avg),]$d_win == 0, 7, 1)

# Select relevant columns
ratings_adj_df <- ratings_adj_df %>% select(year, District, rating_avg)
```

```{r clean polls data, include = FALSE}
district_polls_df <- district_polls %>%
  # Discard special elections etc.
  filter(election_date %in% c("11/8/22", "11/3/20", "11/6/18")) %>%
  # Convert state name to state abbreviation
  mutate(State = state2abbr(state)) %>%
  # Define district names
  mutate(District = paste(State, cd_fips, sep = "-")) %>%
  
  # Calculate days until election
  mutate(end_date = as.Date(end_date, "%m/%d/%y")) %>%
  mutate(days_until_election = 
           case_when(year == 2022 ~ round(difftime("2022-11-8", end_date, units = "days")),
                     year == 2020 ~ round(difftime("2020-11-3", end_date, units = "days")),
                     year == 2018 ~ round(difftime("2018-11-6", end_date, units = "days"))
                     )) %>%
  
  # Discard polls conducted before election year
   mutate(election_year =
           case_when(year == 2022  & end_date > "2022-1-1" ~ 1,
                     year == 2020  & end_date > "2020-1-1" ~ 1,
                     year == 2018  & end_date > "2018-1-1" ~ 1,
                     TRUE ~ 0
                     )) %>%
  filter(election_year == 1) %>%
  
  # Convert to numeric
  mutate(dem_poll = as.numeric(DEM), rep_poll = as.numeric(REP)) %>%
  # Discard rows with NA values
  drop_na(dem_poll, rep_poll) %>%
  # Calculate lead
  mutate(dem_lead = dem_poll - rep_poll) %>%
  # add weights
  mutate(recency = 365 - as.numeric(days_until_election))

district_polls_df <- district_polls_df %>%
  # calculate weighted average
  group_by(year, District) %>%
  summarize(dem_lead_weighted = weighted.mean(dem_lead, recency)) %>%
  select(year, District, dem_lead_weighted)
```

```{r join data sets, include = FALSE}
# 2018 District-level data
df_district_2018 <- 
  full_join(vote_share_df, ratings_adj_df, by = c("year", "District")) %>%
  filter(year == 2018) %>%
  left_join(incumb_df, by = c("year", "District")) %>%
  left_join(district_polls_df, by = c("year", "District")) %>%
  left_join(clinton_df)

# 2014 District-level data
df_district_2014 <- 
  full_join(vote_share_df, ratings_adj_df, by = c("year", "District")) %>%
  filter(year == 2014) %>%
  left_join(incumb_df, by = c("year", "District")) %>%
  left_join(district_polls_df, by = c("year", "District")) %>%
  left_join(obama_df)

# Combine those two dataframes
df_district <- bind_rows(df_district_2018, df_district_2014)

# Democratic popular vote share in each year
df_district <- df_district %>%
  mutate(d_pop_share =
         case_when(
            year == 2014 ~ pop_share_df$d_pop_share[pop_share_df$year == 2014],
            year == 2018 ~ pop_share_df$d_pop_share[pop_share_df$year == 2018]
            ))

# Democratic vote share in each district relative to Democratic popular vote share
df_district <- df_district %>% 
  mutate(pct_D_rel = pct_D/d_pop_share) 
```

```{r df_polls_not_available, include = FALSE}
# Polling not available
df_polls_not_available <- df_district %>% filter((is.na(dem_lead_weighted) == TRUE))
```

```{r df_polls_available, include = FALSE}
# Polling available
df_polls_available <- df_district %>% drop_na(dem_lead_weighted)
```

```{r model_district_1, include = FALSE}
model_district_1 <- 
  lm(pct_D_rel ~ 
       prev_pres_d + rating_avg + d_inc,
       data = df_district)
```

```{r model_district_2, include = FALSE}
model_district_2 <- 
  lm(pct_D_rel ~  
       dem_lead_weighted + # Polls
       prev_pres_d + d_inc,
       data = df_polls_available)
```

```{r calculate rmse for model_district_1, include = FALSE}
rmse_model_district_1 <- (model_district_1$model$pct_D_rel - 
                            model_district_1$fitted.values)^2 %>%
  mean() %>% sqrt()
```

```{r calculate rmse for model_district_2, include = FALSE}
rmse_model_district_2 <- (model_district_2$model$pct_D_rel - 
                            model_district_2$fitted.values)^2 %>%
  mean() %>% sqrt()
```

```{r cross-validation model_district_2, include = FALSE}
set.seed(2022)
cv_model_district_2_error <- 
  sapply(1:1000, function(i){
    # Randomly hold out 30 observations
    obs_outsamp <- sample(nrow(df_polls_available), 30) 
    
    # Linear regression based on the remaining observations
    outsamp_mod <- lm(pct_D_rel ~ dem_lead_weighted + prev_pres_d + d_inc, 
                    df_polls_available[-obs_outsamp,])
    
    # Make predictions based on the model
    outsamp_pred <- 
      predict(outsamp_mod,
              df_polls_available[-obs_outsamp,])
    
    # True values
    outsamp_true <- df_polls_available[-obs_outsamp,]$pct_D_rel
    
    sqrt(mean((outsamp_pred - outsamp_true)^2))
})
```

```{r clean 2022 expert ratings data, include = FALSE}
ratings_2022_df <- ratings_2022 %>%
  mutate(Cook = case_when(
    Cook == "Solid Democratic" ~ 1,
    Cook == "Likely Democratic" ~ 2,
    Cook == "Lean Democratic" ~ 3,
    Cook == "Toss-up" ~ 4,
    Cook == "Lean Republican" ~ 5,
    Cook == "Likely Republican" ~ 6,
    Cook == "Solid Republican" ~ 7
  )) %>%
  mutate(Inside = case_when(
    Inside == "Solid Democratic" ~ 1,
    Inside == "Likely Democratic" ~ 1.75,
    Inside == "Lean Democratic" ~ 2.5,
    Inside == "Tilt Democratic" ~ 3.25,
    Inside == "Toss-up" ~ 4,
    Inside == "Tilt Republican" ~ 4.75,
    Inside == "Lean Republican" ~ 5.5,
    Inside == "Likely Republican" ~ 6.25,
    Inside == "Solid Republican" ~ 7
  )) %>%
  mutate(Sabato = case_when(
    Sabato == "Safe Democratic" ~ 1,
    Sabato == "Likely Democratic" ~ 2,
    Sabato == "Lean Democratic" ~ 3,
    Sabato == "Toss-up" ~ 4,
    Sabato == "Lean Republican" ~ 5,
    Sabato == "Likely Republican" ~ 6,
    Sabato == "Safe Republican" ~ 7
  ))

# Calculate ratings average
ratings_2022_df$rating_avg = rowMeans(ratings_2022_df[, -1])
ratings_2022_df <- ratings_2022_df %>%
  select(District, rating_avg)
```

```{r update expert ratings data 1, include = FALSE}
ratings_latest_df <- ratings_2022_10_27 %>%
  # District numbers
  mutate(district = str_pad(district, 2, pad = "0")) %>%
  # Convert state name to state abbreviation
  mutate(State = state2abbr(state)) %>%
  # Define district names
  mutate(District = paste(State, district, sep = "-")) %>%
  select(District, cook, rothenberg, sabatos_crystal_ball)

# Change district names
ratings_latest_df$District[which(ratings_latest_df$District == "AK-AL")] <- "AK-01"

# Calculate ratings average
ratings_latest_df$rating_avg = rowMeans(ratings_latest_df[, -1])
ratings_latest_df <- ratings_latest_df %>%
  select(District, rating_avg)
```

```{r update expert ratings data 2, include = FALSE}
rating_2022_df_updated <- 
  bind_rows(
    # Retain old data if new data is not available
    anti_join(ratings_2022_df, ratings_latest_df, by = "District"),
    # Update if new data is available
    ratings_latest_df
  )
```

```{r clean incumbency data 2022, include = FALSE}
incumb_2022_df <- incumb_2022 %>%
  # Convert state name to state abbreviation
  mutate(State = state2abbr(state)) %>%
  # District numbers
  mutate(District = str_pad(district, 2, pad = "0")) %>%
  # Define district names
  mutate(District = paste(State, District, sep = "-"))

# Change district names
incumb_2022_df$District[which(incumb_2022_df$District == "AK-AL")] <- "AK-01"
incumb_2022_df$District[which(incumb_2022_df$District == "DE-AL")] <- "DE-01"
incumb_2022_df$District[which(incumb_2022_df$District == "WY-AL")] <- "WY-01"
incumb_2022_df$District[which(incumb_2022_df$District == "VT-AL")] <- "VT-01"
incumb_2022_df$District[which(incumb_2022_df$District == "ND-AL")] <- "SD-01"
incumb_2022_df$District[which(incumb_2022_df$District == "SD-AL")] <- "SD-01"

# Democratic incumbent running for re-election
incumb_2022_df <- incumb_2022_df %>%
  mutate(d_inc = case_when(cand_party == "Democratic" & incumbent == 1 ~ 1, 
                           TRUE ~ 0)) %>%
  filter(d_inc == 1) %>%
  # Select relevant columns
  select(District, d_inc) 
```

```{r polls data 2022, include = FALSE}
district_polls_df_2022 <- district_polls_df %>% filter(year == 2022) 
```

```{r 2020 Biden performance, include = FALSE}
biden_df <- pres_vote_share_2020 %>%
  mutate(prev_pres_d = 100*Dem_2020_Pres/(Dem_2020_Pres + Rep_2020_Pres)) %>%
  select(District, prev_pres_d)
```

```{r merge 2022 data, include = FALSE}
df_2022 <- rating_2022_df_updated %>%
  full_join(incumb_2022_df, by = "District") %>%
  full_join(biden_df, by = "District") %>%
  left_join(district_polls_df_2022, by = "District")

# d_inc = 0 if no Democratic incumbent
df_2022$d_inc[is.na(df_2022$d_inc)] <- 0

# Poll available
df_2022_poll <- df_2022 %>% drop_na(dem_lead_weighted)
# No poll
df_2022_no_poll <- setdiff(df_2022, df_2022_poll) 
```

```{r district-level prediction based on model_district_1, include = FALSE}
df_2022_no_poll_predict <- df_2022_no_poll
# Democrat's predicted performance relative to national popular vote share
df_2022_no_poll_predict <- 
  bind_cols(df_2022_no_poll_predict["District"],
            predict(model_district_1, df_2022_no_poll_predict, interval = "prediction"))
```

```{r district-level prediction based on model_district_2, include = FALSE}
df_2022_poll_predict <- df_2022_poll
# Democrat's predicted performance relative to national popular vote share
df_2022_poll_predict <- 
  bind_cols(df_2022_poll_predict["District"],
            predict(model_district_2, df_2022_poll_predict, interval = "prediction"))
```

```{r model_district_1 prediction, include = FALSE}
d_vote_share_predict_no_poll <- 
  matrix(NA, nrow = nrow(df_2022_no_poll_predict), ncol = nsims)

# Democrat's predicted performance relative to national popular vote share
for(i in 1:nrow(df_2022_no_poll_predict)){
  for(j in 1:nsims){
      d_vote_share_predict_no_poll[i, j] <-
        # Predicted values
        (df_2022_no_poll_predict$fit[i] +
        # Add a normally distributed disturbance
        rnorm(n = 1, mean = 0, sd = rmse_model_district_1))*
        # Outcome of simulations (national popular vote)
        model_national_1_pred_w_error[j]
  }
}

# Combine df's
no_poll_predict <- 
  bind_cols(select(df_2022_no_poll_predict, c("District", "fit", "lwr", "upr")),
            as_tibble(d_vote_share_predict_no_poll))
```


```{r model_district_2 prediction, include = FALSE}
d_vote_share_predict_poll <- 
  matrix(NA, nrow = nrow(df_2022_poll_predict), ncol = nsims)

# Democrat's predicted performance relative to national popular vote share
for(i in 1:nrow(df_2022_poll_predict)){
  for(j in 1:nsims){
      d_vote_share_predict_poll[i, j] <-
        # Predicted values
        (df_2022_poll_predict$fit[i]+
        # Add a normally distributed disturbance
        rnorm(n = 1, mean = 0, sd = rmse_model_district_2))*
        # Outcome of the simulations (national popular vote)
        model_national_1_pred_w_error[j]
  }
}

# Combine df's
poll_predict <- 
  bind_cols(select(df_2022_poll_predict, c("District", "fit", "lwr", "upr")),
            as_tibble(d_vote_share_predict_poll))
```

```{r simulated outcomes, include = FALSE}
# Combine prediction based on model_district_1 with prediction based on model model_district_2
poll_no_poll <- bind_rows(no_poll_predict, poll_predict) %>%
  select(-c("fit", "lwr", "upr"))

# Save data
# write.csv(poll_no_poll, "district_dem_share_pred.csv")
# write.csv(no_poll_predict, "district_dem_share_pred_model2a.csv")
# write.csv(poll_predict, "district_dem_share_pred_model2b.csv")

poll_no_poll_count <- bind_rows(no_poll_predict, poll_predict) %>%
  select(-c("District", "fit", "lwr", "upr"))

dem_seats_simulation <- rep(NA, nsims)
for(i in 1:nsims){
  dem_seats_simulation[i] <-
    # For each simulation, 
    # count the number of races where Democrats are projected to win
    sum(poll_no_poll_count[,i] >= 50) 
}
```

In this blog post, I will assess the accuracy of my models and discuss why my models were wrong in some respects. The data was last updated in the morning of Monday, November 21.


## 1. Recap of Models and Predictions

Inspired by Bafumi et al. (2018), I built a forecast model made up of two steps. First, I predicted the Democratic popular vote share and simulated 5,000 potential outcomes at the national level (Model 1). I obtained a set of 5,000 draws from a normal distribution whose mean is the predicted Democratic two-party popular vote share and whose standard deviation is the RMSE of Model 1. Next, to forecast the district-level vote share, I predicted the ratio between the Democratic vote share in a certain district in election t and the Democratic nation-wide vote share in election t (Model 2a, 2b). By multiplying the predicted value of this ratio in each district by the 5,000 simulated Democratic popular vote shares obtained in the first step, I obtained a set of 5,000 potential Democratic vote shares for each district. When multiplying the predicted value of Model2a/2b by the predicted value of Model 1, I added a disturbance drawn from a normal distribution whose mean is 0 and whose standard deviation is the RMSE of Model 2a/2b.


The equation for **Model 1** is:

![](model_formula_1.png)

The generic ballot and whether the election was a midterm election were the only independent variables.

**Model 2a** was used for districts without polling data:

![](model_formula_2a.png)

In lieu of polling data, I used the average of the ratings determined by the Cook Political Report, Inside Elections, and Sabato’s Crystal Ball.

**Model 2b** was used for districts with polling:

![](model_formula_2b.png)

Models 2a/2b were pooled models. I took into account redistricting by incorporating data on Joe Biden’s vote share aggregated to the post-redistricting district-level.

Using Model 1, I predicted that the Democratic two-party popular vote share would be 48.52 (Prediction interval: [45.36, 51.68]). Democrats were predicted to lose the popular vote in 4251 out of the 5000 simulations (Figure 1). As for the district-level prediction, I predicted a landslide victory for the Republicans, with the most probable outcome being Democrats winning 198 seats (Figure 2). Democrats were predicted to win the majority of seats in only 427 out of the 5,000 simulations.


## 2. Accuracy & Patterns in the Inaccuracy

### Model 1: Democratic Popular Vote Share

```{r actual results, include = FALSE}
actual_vote_share_names <- read_csv('analysis_data/2022_4_0_3.csv', n_max = 0) %>% names()
dat <- read_csv("analysis_data/2022_4_0_3.csv", col_names = actual_vote_share_names, skip = 2)
```

```{r hand enter MA, ME, and MS, include = FALSE}
# Source: https://www.nytimes.com/interactive/2022/11/08/us/elections/results-house.html?action=click&pgtype=Article&state=default&module=election-results&context=election_recirc&region=NavBar
##As of 11/21/22
dat[dat$FIPS == 25901,'Democratic'] = 153402; dat[dat$FIPS == 25901,'Republican'] = 96499
dat[dat$FIPS == 25902,'Democratic'] = 178472; dat[dat$FIPS == 25902,'Republican'] = 91100
dat[dat$FIPS == 25903,'Democratic'] = 145507; dat[dat$FIPS == 25903,'Republican'] = 82628
dat[dat$FIPS == 25904,'Democratic'] = 0; dat[dat$FIPS == 25904,'Republican'] = 0
dat[dat$FIPS == 25905,'Democratic'] = 198617; dat[dat$FIPS == 25905,'Republican'] = 70694
dat[dat$FIPS == 25906,'Democratic'] = 190062; dat[dat$FIPS == 25906,'Republican'] = 107496
dat[dat$FIPS == 25907,'Democratic'] = 144902; dat[dat$FIPS == 25907,'Republican'] = 26481
dat[dat$FIPS == 25908,'Democratic'] = 184084; dat[dat$FIPS == 25908,'Republican'] = 80961
dat[dat$FIPS == 25909,'Democratic'] = 193426; dat[dat$FIPS == 25909,'Republican'] = 131936

dat[dat$FIPS == 23901,'Democratic'] = 218630; dat[dat$FIPS == 23901,'Republican'] = 128996
dat[dat$FIPS == 23902,'Democratic'] = 151440; dat[dat$FIPS == 23902,'Republican'] = 140895

dat[dat$FIPS == 28901,'Democratic'] = 45222; dat[dat$FIPS == 28901,'Republican'] = 122122
dat[dat$FIPS == 28902,'Democratic'] = 107071; dat[dat$FIPS == 28902,'Republican'] = 71380
dat[dat$FIPS == 28903,'Democratic'] = 54422; dat[dat$FIPS == 28903,'Republican'] = 132269
dat[dat$FIPS == 28904,'Democratic'] = 42876; dat[dat$FIPS == 28904,'Republican'] = 127813

##LA and FL races with no reporting b/c no contest
dat[dat$FIPS == 22904,'Democratic'] = 0; dat[dat$FIPS == 22904,'Republican'] = 0
dat[dat$FIPS == 12905,'Democratic'] = 0; dat[dat$FIPS == 12905,'Republican'] = 0
```

```{r clean actual_vote_share, include = FALSE}
actual_vote_share_df <- dat %>%
  mutate(state_abbrev = fips(STATE_FIPS, to = "Abbreviation")) %>%
  mutate(District = paste(state_abbrev, CD, sep = "-")) %>%
  select(District, `Total Vote`, Democratic, Republican) %>%
  rename(total_actual = `Total Vote`, dem_actual = Democratic, rep_actual = Republican) %>%
  mutate(actual_dem_share = 100*dem_actual/(dem_actual+rep_actual)) %>%
  mutate(d_win_actual = if_else(dem_actual > rep_actual, 1, 0))

# Alaska-01
# Not called yet but predicted to be won by Peltola (D)
actual_vote_share_df$d_win_actual[actual_vote_share_df$District == "AK-01"] <- 1
# MA-04: Uncontested
actual_vote_share_df$d_win_actual[actual_vote_share_df$District == "MA-04"] <- 1
```

```{r actual two-party vote share, include = FALSE}
actual_dem_pop_share <- 100*sum(actual_vote_share_df$dem_actual)/(sum(actual_vote_share_df$dem_actual)+sum(actual_vote_share_df$rep_actual))
```


I overpredicted the Democratic two-party popular vote share by 0.41 points. This error is relatively small compared to the magnitude of the errors of the 10 models that political scientists built prior to the 2016 Presidential elections. The errors of those 10 models ranged from 0.1 to 3.6 (Campbell et al., 2017). The high accuracy of Model 1 may appear surprising given that it was only based on two variables: the generic ballot and the midterm penalty. This is yet another evidence that the generic ballot is highly predictive of electoral outcomes, as it has been suggested repeatedly (Abramowitz, 2018; Bafumi et al, 2018).


```{r plot predicted dem popular vote share, echo = FALSE}
ggplot(mapping = aes(model_national_1_pred_w_error)) +
  geom_histogram(binwidth = 0.5, boundary = 50) +
  geom_vline(aes(xintercept = model_national_1_pred[1]), color = "blue") +
  geom_vline(aes(xintercept = actual_dem_pop_share), color = "cyan") +
  geom_vline(aes(xintercept = 50), color = "purple", linetype = "dashed") +
  annotate(geom = "text",
           x =  model_national_1_pred[1]+1.0, y = 715, 
           label = paste("Predicted:",  round(model_national_1_pred[1], 2)), color = "blue") +
  annotate(geom = "text",
           x =  actual_dem_pop_share-1.0, y = 715, 
           label = paste("Actual:",  round(actual_dem_pop_share, 2)), color = "cyan") +
  labs(title = "Figure 1: Democratic Popular Vote Share (5,000 Simulations)",
        x = "Two-Party Democratic Popular Vote Share", y = "Frequency") +
  theme_light()
```

### Model 2: Democratic Seat Share

```{r actual_dem_seats, include = FALSE}
# Source: https://www.nytimes.com/interactive/2022/11/10/us/elections/results-house-seats-elections-congress.html?action=click&pgtype=Article&state=default&module=election-results&context=election_recirc&region=NavBar
actual_dem_seats <- 213
```

The district-level forecast was less accurate. Democrats are currently projected to win 213 seats, which is more than the 198 that I had predicted (Figure 2). Although 213 is within the range of 5,000 potential outcomes, there is no denying that I overpredicted Republican wins at the district-level. My district-level forecast would have been even worse if I had predicted the popular vote share more accurately. Since I multiply the predicted Democratic popular vote share by the predicted value of Model 2a/2b, my district-level forecast would have predicted an even larger Republican landslide if the predicted Democratic popular vote share was lower and closer to the actual outcome. Clearly, there was something wrong with Model 2a/2b.

```{r plot predicted dem seats, echo = FALSE}
ggplot(mapping = aes(dem_seats_simulation)) +
  geom_histogram(binwidth = 2, boundary = 180) +
  geom_vline(aes(xintercept = median(dem_seats_simulation)), color = "blue") +
  # Actual Dem seats
  geom_vline(aes(xintercept = actual_dem_seats), color = "cyan") +
  geom_vline(aes(xintercept = 218), color = "purple", linetype = "dashed") +
  annotate(geom = "text",
           x =  median(dem_seats_simulation)-13, y = 320, 
           label = paste("Predicted (Median):", median(dem_seats_simulation)), color = "blue") +
  annotate(geom = "text",
           x =  actual_dem_seats+6, y = 290, 
           label = paste("Actual:", actual_dem_seats), color = "cyan") +
  labs(title = "Figure 2: Number of Seats Won By Democrats (5,000 Simulations)",
        x = "Number of Democratic Seats", y = "Frequency",
       caption = "Actual: 212 + Alaska ('Path to 218: Tracking the Remaining House Races')") +
  theme_light()
```

```{r district-level prediction, include = FALSE}
district_level_pred <- poll_no_poll %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  group_by(District) %>%
  mutate(median_pct_D = median(pred)) %>%
  filter(name == "V1") %>%
  select(-c(name, pred)) %>%
  mutate(d_win_pred = if_else(median_pct_D > 50, 1, 0))

# Model 2a: No polls
district_level_pred_model2a <- no_poll_predict %>%
  select(-c("fit", "lwr", "upr")) %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  group_by(District) %>%
  mutate(median_pct_D = median(pred)) %>%
  filter(name == "V1") %>%
  select(-c(name, pred)) %>%
  mutate(d_win_pred = if_else(median_pct_D > 50, 1, 0))

# Model 2b: With polls
district_level_pred_model2b <- poll_predict %>%
  select(-c("fit", "lwr", "upr")) %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  group_by(District) %>%
  mutate(median_pct_D = median(pred)) %>%
  filter(name == "V1") %>%
  select(-c(name, pred)) %>%
  mutate(d_win_pred = if_else(median_pct_D > 50, 1, 0))
```


```{r compare_pred_actual, include = FALSE}
compare_pred_actual <- actual_vote_share_df %>%
  select(District, actual_dem_share, d_win_actual) %>%
  full_join(district_level_pred, by = "District") %>%
  mutate(false_dem = if_else(d_win_pred == 1 & d_win_actual == 0, 1, 0)) %>%
  mutate(false_rep = if_else(d_win_pred == 0 & d_win_actual == 1, 1, 0))

# Model 2a: No polls
compare_pred_actual_model2a <- actual_vote_share_df %>%
  select(District, actual_dem_share, d_win_actual) %>%
  right_join(district_level_pred_model2a, by = "District") %>%
  mutate(false_dem = if_else(d_win_pred == 1 & d_win_actual == 0, 1, 0)) %>%
  mutate(false_rep = if_else(d_win_pred == 0 & d_win_actual == 1, 1, 0))

# Model 2b: With polls
compare_pred_actual_model2b <- actual_vote_share_df %>%
  select(District, actual_dem_share, d_win_actual) %>%
  right_join(district_level_pred_model2b, by = "District") %>%
  mutate(false_dem = if_else(d_win_pred == 1 & d_win_actual == 0, 1, 0)) %>%
  mutate(false_rep = if_else(d_win_pred == 0 & d_win_actual == 1, 1, 0))
```


```{r compare pred vs actual analysis, include = FALSE}
# Number of Dem seats
# Model 2a
dem_pred_model2a <- sum(compare_pred_actual_model2a$d_win_pred == 1)
dem_actual_model2a <- sum(compare_pred_actual_model2a$d_win_actual == 1)
# Model 2b
dem_pred_model2b <- sum(compare_pred_actual_model2b$d_win_pred == 1)
dem_actual_model2b <- sum(compare_pred_actual_model2b$d_win_actual == 1)

# Percent called correctly
percent_pred_correctly <- 
  sum(compare_pred_actual$d_win_actual == compare_pred_actual$d_win_pred)/435
# Incorrectly predicted Dem victory
false_dem_district <- compare_pred_actual %>%
  filter(false_dem == 1)
# Incorrectly predicted Rep victory
false_rep_district <- compare_pred_actual %>%
  filter(false_rep == 1)

# Percent predicted correctly: Model 2a (without polls)
percent_pred_correctly_model2a <- 
  sum(compare_pred_actual_model2a$d_win_actual == compare_pred_actual_model2a$d_win_pred)/
  nrow(compare_pred_actual_model2a)
# Incorrectly predicted Dem victory
false_dem_district_model2a <- compare_pred_actual_model2a %>%
  filter(false_dem == 1)
# Incorrectly predicted Rep victory
false_rep_district_model2a <- compare_pred_actual_model2a %>%
  filter(false_rep == 1)

# Percent predicted correctly: Model 2a (with polls)
percent_pred_correctly_model2b <- 
  sum(compare_pred_actual_model2b$d_win_actual == compare_pred_actual_model2b$d_win_pred)/
  nrow(compare_pred_actual_model2b)
# Incorrectly predicted Dem victory
false_dem_district_model2b <- compare_pred_actual_model2b %>%
  filter(false_dem == 1)
# Incorrectly predicted Rep victory
false_rep_district_model2b <- compare_pred_actual_model2b %>%
  filter(false_rep == 1)
```

```{r rmse model 2, include = FALSE}
# Overall
error_pred_actual <- compare_pred_actual %>%
  # Drop uncontested
  filter(District %in%   
           c("AZ-08", "PA-15", "AZ-09", "SC-03", "FL-05", "SC-04", "IL-07", 
             "TX-06", "LA-04", "TX-11", "MA-04", "TX-25", "NY-13", "TX-31", 
             "PA-13", "WI-06", "PA-14", "AL-01", "NY-09", "AL-06", "PA-03", 
             "CA-10", "SD-01", "FL-06", "TX-19", "FL-18", "TX-26", "LA-06", 
             "WI-08", "ND-01", "CA-15", "CA-30", "CA-16", "CA-34", "CA-29", 
             "CA-37") == FALSE) %>%
  mutate(error = actual_dem_share - median_pct_D) %>%
  pull(error)

# Calculate RMSE
rmse_pred_actual <- sqrt(mean(error_pred_actual^2))

# Model 2a: No polls
error_pred_actual_model2a <- compare_pred_actual_model2a %>%
  # Drop uncontested
  filter(District %in%   
           c("AZ-08", "PA-15", "AZ-09", "SC-03", "FL-05", "SC-04", "IL-07", 
             "TX-06", "LA-04", "TX-11", "MA-04", "TX-25", "NY-13", "TX-31", 
             "PA-13", "WI-06", "PA-14", "AL-01", "NY-09", "AL-06", "PA-03", 
             "CA-10", "SD-01", "FL-06", "TX-19", "FL-18", "TX-26", "LA-06", 
             "WI-08", "ND-01", "CA-15", "CA-30", "CA-16", "CA-34", "CA-29", 
             "CA-37") == FALSE) %>%
  mutate(error = actual_dem_share - median_pct_D) %>%
  pull(error)

# Caculate RMSE
rmse_pred_actual_model2a <- sqrt(mean(error_pred_actual_model2a^2))

# Model 2b: With polls
error_pred_actual_model2b <- compare_pred_actual_model2b %>%
  filter(District %in%   
           c("AZ-08", "PA-15", "AZ-09", "SC-03", "FL-05", "SC-04", "IL-07", 
             "TX-06", "LA-04", "TX-11", "MA-04", "TX-25", "NY-13", "TX-31", 
             "PA-13", "WI-06", "PA-14", "AL-01", "NY-09", "AL-06", "PA-03", 
             "CA-10", "SD-01", "FL-06", "TX-19", "FL-18", "TX-26", "LA-06", 
             "WI-08", "ND-01", "CA-15", "CA-30", "CA-16", "CA-34", "CA-29", 
             "CA-37") == FALSE) %>%
  mutate(error = actual_dem_share - median_pct_D) %>%
  pull(error)

# Caculate RMSE
rmse_pred_actual_model2b <- sqrt(mean(error_pred_actual_model2b^2))
```

```{r brier score, include = FALSE}
# Calculate predicted probabilities
brier_data <- poll_no_poll %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  # 1 if Dem victory predicted
  mutate(d_win = if_else(pred > 50, 1, 0)) %>%
  group_by(District) %>%
  summarize(d_win_prob = mean(pred)/100)

# Calculate Brier score
brier_score_df <- brier_data %>%
  full_join(compare_pred_actual, by = "District") %>%
  select(District, d_win_prob, d_win_actual) %>%
  mutate(brier = (d_win_prob - d_win_actual)^2) 

brier_score = mean(brier_score_df$brier)  

# Do the same for Model 2a (No polls)
brier_data_model2a <- no_poll_predict %>%
  select(-c("fit", "lwr", "upr")) %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  # 1 if Dem victory predicted
  mutate(d_win = if_else(pred > 50, 1, 0)) %>%
  group_by(District) %>%
  summarize(d_win_prob = mean(pred)/100)

# Calculate Brier score
brier_score_df_model2a <- brier_data_model2a %>%
  full_join(compare_pred_actual_model2a, by = "District") %>%
  select(District, d_win_prob, d_win_actual) %>%
  mutate(brier = (d_win_prob - d_win_actual)^2) 

brier_score_model2a = mean(brier_score_df_model2a$brier)  

# Model 2b: With polls
brier_data_model2b <- poll_predict %>%
  select(-c("fit", "lwr", "upr")) %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  # 1 if Dem victory predicted
  mutate(d_win = if_else(pred > 50, 1, 0)) %>%
  group_by(District) %>%
  summarize(d_win_prob = mean(pred)/100)

# Calculate Brier score
brier_score_df_model2b <- brier_data_model2b %>%
  full_join(compare_pred_actual_model2b, by = "District") %>%
  select(District, d_win_prob, d_win_actual) %>%
  mutate(brier = (d_win_prob - d_win_actual)^2) 

brier_score_model2b = mean(brier_score_df_model2b$brier)  
```

```{r predicted probabilities, include = FALSE}
summary(brier_data_model2a$d_win_prob)
summary(brier_data_model2b$d_win_prob)
```

Table 1 suggests that Model 2b, which is the model that was used for districts with polling data, is responsible for the large error at the district level. Model 2a, which is the model for districts without polling data, correctly predicted the winner in over 99% of the races while model 2b predicted the winner correctly in only 61.3% of the races. Although it may seem counterintuitive that the availability of polls made the predictions worse, Model 2b’s low classification accuracy makes sense after considering the types of districts for which the models were used. Model 2a was used for districts without polling data, which are districts that are less competitive and more predictable. Since district-level polling tends to be conducted in competitive districts, the districts for which Model 2b was used were harder to predict by nature. This is reflected in the Brier scores. Since Model 2a tended to yield a predicted probability of a Democratic victory closer to 0 or 1, the Brier score is low. Meanwhile, the predicted probability of a Democratic victory based on Model 2b ranged from 0.395 to 0.594, so the Brier score is higher. (In this case, the predicted probability of a Democratic victory in a district is the share of the 5,000 simulations that resulted in a Democratic victory.)

Table 1 also shows that the RMSE was lower for Model 2b. Although Model 2a accurately predicted the winner over 99% of the time, the error of the vote share tended to be large (Figure 3). The error of Model 2b tended to be small but tended to be positive, which reflects the fact that Model 2b overpredicted Republican victories (Figure 4). Overall, these analyses suggest that Model 2a predicted the winner better than Model 2b but that Model 2b predicted the exact Democratic vote share more accurately.

```{r summary table prep, include = FALSE}
summary_table <- 
  tribble(~category, ~overall, ~model2a, ~model2b,
        "Dem Seats Predicted", 198, dem_pred_model2a, dem_pred_model2b,
        "Dem Seats Actual", 213, dem_actual_model2a, dem_actual_model2b,
        "Classification Accuracy", 
          percent_pred_correctly, percent_pred_correctly_model2a, 
          percent_pred_correctly_model2b,
        "Vote Share RMSE", 
          rmse_pred_actual, rmse_pred_actual_model2a, 
          rmse_pred_actual_model2b,
        "Brier Score", brier_score, brier_score_model2a, brier_score_model2b)
```

```{r summary table, echo = FALSE}
summary_table %>%
  gt(rowname_col = "category") %>%
  fmt_number(columns = everything(), rows = 1:2, decimals = 0) %>%
  fmt_number(columns = everything(), rows = 3:5, decimals = 3) %>%
  tab_header(title = "Table 1: Summary of Models") %>%
  cols_label(
    overall = "Overall",
    model2a = "Model 2a (Without Polls)",
    model2b = "Model 2b (With Polls)",
  ) %>%
  tab_footnote(
    footnote = "Excluding uncontested races",
    locations = cells_body(
      columns = 2:4,
      rows = 4
    ) 
  )  %>%
  tab_footnote(
    footnote = paste("Predicted R/Actual D:", 
                      paste(false_rep_district_model2a$District, collapse = ", ")),
    locations = cells_body(
      columns = 3,
      rows = 3
    ) 
  )  %>%
 tab_footnote(
    footnote = paste("Predicted R/Actual D:", 
                      paste(false_rep_district_model2b$District, collapse = ", ")),
    locations = cells_body(
      columns = 4,
      rows = 3
    ) 
  )  %>%
  tab_footnote(
    footnote = paste("Predicted D/Actual R:", 
                      paste(false_dem_district_model2b$District, collapse = ", ")),
    locations = cells_body(
      columns = 4,
      rows = 3
    ) 
  )  %>%
  tab_footnote(
    footnote = "212 + Alaska ('Path to 218: Tracking the Remaining House Races')",
    locations = cells_body(
      columns = 2,
      rows = 2
    ) 
  )  %>%
  opt_footnote_marks(marks = "letters") 
```

```{r model 2a error histogram, echo = FALSE}
ggplot(mapping = aes(error_pred_actual_model2a)) +
  geom_histogram(binwidth = 2, boundary = 180) +
  labs(title = "Figure 3: Model 2a (Wihout Polls) Error",
        x = "Actual - Predicted Dem Vote Share", y = "Frequency") +
  theme_light()
```
```{r model 2b error histogram, echo = FALSE}
ggplot(mapping = aes(error_pred_actual_model2b)) +
  geom_histogram(binwidth = 2, boundary = 180) +
  labs(title = "Figure 4: Model 2b (With Polls) Error",
        x = "Actual - Predicted Dem Vote Share", y = "Frequency") +
  theme_light()
```

```{r cd2022 map, include = FALSE}
cd2022_raw <- st_read("analysis_data/cd2022/HexCDv30.shp")
cd2022 <- cd2022_raw %>%
  mutate(CDLABEL =  str_pad(CDLABEL, 2, pad = "0")) %>%
  # District names
  mutate(District = paste(STATEAB, CDLABEL, sep = "-"))
```

```{r districts missed, include = FALSE}
cd2022_missed <- cd2022 %>%
  mutate(missed = case_when(
    # Predicted D; Actual: R 
    District %in% false_dem_district$District ~ 2,
    # Predicted R; Actual: D
    District %in% false_rep_district$District ~ 1,
    TRUE ~ 0
  ))
```


There was no clear pattern in terms of the locations where the predictions were inaccurate (Figure 5). However, it is interesting to note that the predictions were wrong in several districts in three battleground Midwestern states: Ohio, Michigan, Minnesota. Midwestern states have been battleground states in recent presidential elections even as states in other states have become more reliably blue or red. Although it is unclear the extent to which this applies to House elections, Hopkins (2017) argues that the Midwest is a rare area in which a sizable portion of the electorate is open to persuasion by campaigns (pp. 202-207).

```{r plot districts missed, echo = FALSE}
ggplot(cd2022_missed) +
  geom_sf(aes(fill = as.factor(missed))) +
  scale_fill_manual(values = c("grey80", "blue", "red"),
                    labels = c("Correct", 
                               "Predicted R; Actual D",
                               "Predicted D; Actual R")) +
  
 labs(title = "Figure 5: Districts Missed",
       caption = "Shapefile: Donner (2022)") +
    
 # Set theme
 theme(legend.position = "bottom",
       axis.line = element_blank(), axis.text = element_blank(),
       axis.ticks = element_blank(), axis.title = element_blank(),
       legend.title = element_blank(),
       panel.background = element_blank())
```

```{r competitive districts, include = FALSE}
competitive_pred <- poll_no_poll %>%
  filter(District %in% 
           c("KS-03", "AK-01", "PA-07", "NY-19", "VA-02", "RI-02", "TX-15", "MI-03",
             "CA-22", "CA-27", "IL-17", "IA-03", "OR-05", "NM-02", "NY-18", "AZ-02",
             "ME-02", "PA-08", "NV-03", "PA-17", "NY-03", "MD-06", "CA-13", "OR-06",
             "TX-34", "NJ-07", "NY-22", "MN-02", "MI-07", "OH-09")
         ) %>%
  pivot_longer(cols = 2:5001, values_to = "pred") %>%
  group_by(District) %>%
  mutate(median_pct_D = mean(pred)) 
```

```{r ca-22 actual, include = FALSE}
ca_22_actual_dem_share <- 
  # Source: https://www.nytimes.com/interactive/2022/11/08/us/elections/results-california-us-house-district-22.html
   100*48461/(48461+51842)
```


### CA-22

California’s 22nd District ended up being extremely competitive, and the race was not called until late at night Eastern Time on Monday November 21. Indeed, I had expected that the race would be close: the predicted Democratic vote share was 49.78 and 2700 out of the 5000 simulations predicted a Republican victory. As of 23:30 on November 21, Rudy Salas (D) has won 48.31% of the votes counted. Although there are still ballots that have yet to be counted, it is safe to say that my prediction that the race would be extremely close but that David Valadao (R) has a very narrow advantage turned out to be accurate.

```{r ca-22, echo = FALSE}
ca_22 <- competitive_pred %>%
  filter(District == "CA-22")

ggplot(data = ca_22) +
  geom_histogram(aes(x = pred), binwidth = 1) +
  # Red line if median_pct_D < 50
  geom_vline(aes(xintercept = median_pct_D), color = "red",
             data = ca_22[ca_22$median_pct_D < 50,]) +
  geom_vline(xintercept = ca_22_actual_dem_share, color = "violet") +
  annotate(geom = "text",
           x =  ca_22$median_pct_D[1]+3.3, y = 700, 
           label = paste0("Predicted: ", round(ca_22$median_pct_D[1], 2)), color = "red") +
  annotate(geom = "text",
           x =  ca_22_actual_dem_share-2.0, y = 700, 
           label = paste0("Actual: ", round(ca_22_actual_dem_share, 2)), color = "violet") +
  labs(title = "Figure 6: Actual vs Predicted Democratic Vote Share in CA-22",
       subtitle = "5,000 Simulations",
       x = "Democratic Two-Party Vote Share", y = "Frequency",
       caption = "Data updated 11:30pm on 11/21/22 (California 22nd Congressional District Election Results)") +
  theme_bw()
```


## 3. Hypothesis: Why was Model 2 Inaccurate?
Since Model 1 was very accurate, I will discuss why Models 2a/2b were inaccurate.

#### Hypothesis 1: Democratic candidates did well in open seats.

The districts where I failed to accurately predict the outcomes tended to be districts without an incumbent. Out of the 23 districts that Democrats won even though I had predicted that Republicans would win, 17 did not have a Democratic incumbent. Moreover, out of those 17, 10 were open seats (Ballotpedia, 2022). It is possible that Democrats did well in open seats, and this is a factor that I did not take into account in my model. In Models 2a/2b, I coded incumbency as =1 if there was a Democratic incumbent in the district and =0 otherwise. Thus, I did not distinguish open seats from districts where a Republican candidate challenged a Democratic incumbent. Taking into account whether a seat was open was especially important in this election since 7 new seats were created due to redistricting.

```{r analyze false_rep_district 2, include = FALSE}
df_2022 %>% filter(District %in% false_rep_district$District) %>%
  filter(d_inc == 0)
```


#### Hypothesis 2: Trump’s endorsement had a negative impact on Republican candidate’s performance.

Pundits have suggested that low-quality or extreme candidates endorsed by former President Trump tended to underperform in competitive districts and that this could have been the reason why Republicans did not gain as many seats as expected (Cohn, 2022; Wallach, 2022). I did not include candidates’ ideology or Trump’s endorsement in the model, which could have led me to overestimate Republican strength in several districts. Interestingly, in all 5 districts for which Wallach (2022) argues that the “Trump penalty” had a pivotal impact (NC-13, OH-13, OH-9, PA-08, WA-03), my models falsely predicted a Republican victory. 

#### Hypothesis 3: The model was overfit on 2014 and 2018 data.

The key assumption behind Model 2a/2b was that the 2014 and 2018 elections would help us predict candidates’ district-level performance relative to the national popular vote share in 2022. However, it is possible that district-level vote share tends to deviate from the popular election more in some elections than others. The 2014 midterms resulted in a landslide victory for Republicans in the House and the 2018 midterms resulted in a Democratic landslide victory. For these two elections, the ratio between the district-level vote share and the popular vote share could have been larger than in 2022. This could have made the predicted values for Model 2a/2b larger than the actual values. Even though Model 1 suggested that the Republicans have a slight advantage over Democrats, if the magnitude of the predicted values in Model 2a/2b was too large, the Republican advantage at the district level would have been amplified. The risk of overfitting on the training data would have been mitigated if there were more data points in the training data. However, due to data availability issues, I only used data from 2014 and 2018. 

## 4. How to Test Proposed Hypothesis

#### Testing Hypothesis 1: Performance in open seats

To see whether Democrats did well in open seats this election cycle, I could conduct a difference-in-differences test, where the treatment group is open seats and the control group is other seats. If we compare the 2018 district-level Democratic vote share and the 2022 district-level Democratic vote share, on average, the control group most likely experienced a decline between 2018 and 2022 since 2018 was a landslide victory for Democrats. Democratic candidates in open seats on average could have experienced a decline in the vote share, but if this decline is smaller than the decline seen among the control group, that would be in line with my hypothesis that Democrats did relatively well in open seats in 2022. 


#### Testing Hypothesis 2: Trump's endorsement

Pundits have already begun trying to estimate the impact of Trump’s endorsement. Cohn (2022) compared how “MAGA Republicans” (as defined by the Cook Political Report) versus other Republicans performed relative to Trump’s performance in 2020. Traditional Republicans on average performed better relative to Trump’s performance in 2020 while “MAGA Republicans” on average did not do better than Trump. Cohn (2022) concludes that “MAGA Republicans” did worse relative to traditional Republicans by approximately 5 percentage points. Wallach (2022) compares the vote share of candidates endorsed by Trump with the predicted vote share based on the Cook PVI. Focusing on competitive districts, he finds that candidates endorsed by Trump tended to perform worse relative to expectations. Wallach (2022)’s approach seems more plausible because assuming that the expected vote share is accurate and that all relevant variables were included when modelling the expected vote share, the difference between the predicted vote share and the actual vote share can be interpreted as the causal effect of Trump’s endorsement.

#### Testing Hypothesis 3: Overfitting on 2014 and 2018 data

I would need to test whether the ratio between the district-level vote share and the popular vote share tended to be larger in 2014 and 2018 compared to that ratio in 2022 through pairwise t-tests. If the ratio was in fact larger in 2014 and 2018, that would confirm my hypothesis that the predictions for 2022 were affected by the greater deviation of the district-level vote share from the popular vote share in 2014 and 2018.

## 5. How I Might Change My Model

If I were to build a model again, I would address the weaknesses of my model in the following way:

#### Code incumbency as a categorical variable 

Instead of coding incumbency as a binary variable that only reflects whether there is a Democratic incumbent, I could code incumbency as =0 if there is a Republican incumbent, =1 if the seat is an open seat, and =2 if there is a Democratic incumbent. I could add other categories to further distinguish between different incumbency status such as whether an incumbent candidate used to represent a different district but switched districts. Such cases were seen often in 2022 because of redistricting.

#### Include ideology in the model

Including Trump’s endorsement would be difficult because we would have to restrict our training data to elections in which Trump endorsed House candidates. For reference, some suggest that Trump’s endorsement cost as many as 11 House seats in the 2018 midterm elections (Ballard et al., 2021). To allow the training data to be based on more elections, I could include measures of the candidate’s ideology in the model. Literature suggests that nominating extreme candidates in the primaries tends to have a negative impact on the party’s electoral outcomes (Hall, 2015).

#### Expand the scope of the training data

If overfitting on the 2014 and 2018 midterms was indeed an issue, I would consider adding data from earlier elections to the training data. The main issue with this approach is that redistricting happens decennially, and that redistricting could change how competitive districts tend to be. This could affect how candidates tend to do relative to the popular vote share. I would need to see whether incorporating data from two or more redistricting cycles ago is helpful.

### References

Abramowitz, A. (2018). Will Democrats Catch a Wave? The Generic Ballot Model and the 2018 US House Elections. *PS: Political Science & Politics, 51*(S1), 4-6. doi:10.1017/S1049096518001567

Bafumi, J., Erikson, R., & Wlezien, C. (2018). Forecasting the 2018 Midterm Election using National Polls and District Information. *PS: Political Science & Politics, 51*(S1), 7-11. doi:10.1017/S1049096518001579

Ballard, A.O., Hassell, H. J. G., & Heseltine, M. (2021). Be Careful What You Wish For: The Impacts of President Trump’s Midterm Endorsements. *Legislative Studies Quarterly, 46*(2), 459–491. https://doi.org/10.1111/lsq.12284


Ballotpedia. (2022). United States Congress elections, 2022.
https://ballotpedia.org/United_States_Congress_elections,_2022#Open_seats

California 22nd Congressional District Election Results. (2022, November 21). *New York Times.* https://www.nytimes.com/interactive/2022/11/08/us/elections/results-california-us-house-district-22.html

Campbell, J.E., Norpoth, H., Abramowitz, A. I., Lewis-Beck, M. S., Tien, C., Erikson, R. S., Wlezien, C., Lockerbie, B., Holbrook, T. M., Jerôme, B., Jerôme-Speziari, V., Graefe, A., Armstrong, J. S., Jones, R. J., & Cuzán, A. G. (2017). A Recap of the 2016 Election Forecasts. *PS: Political Science & Politics, 50*(2), 331–338. https://doi.org/10.1017/S1049096516002766

Cohn, N. (2022, November 16). Trump’s Drag on Republicans Quantified: A Five-Point Penalty. *New York Times.* https://www.nytimes.com/2022/11/16/upshot/trump-effect-midterm-election.html

Donner, D. (2022). Congressional District Hexmap version 3. *Daily Kos.* https://dkel.ec/map

Hall, A.B. (2015). What happens when extremists win primaries? *American Political Science Review, 109*(1), 18–42.

Hopkins, D.A. (2017). *Red Fighting Blue: How Geography and Electoral Rules Polarize American Politics.* Cambridge University Press. https://doi.org/10.1017/9781108123594

Path to 218: Tracking the Remaining House Races. (2022, November 21). *New York Times.* https://www.nytimes.com/interactive/2022/11/10/us/elections/results-house-seats-elections-congress.html?action=click&pgtype=Article&state=default&module=election-results&context=election_recirc&region=NavBar

Wallach, P. (2022). We can now quantify Trump’s sabotage of the GOP’s House dreams. *Washington Post.* https://www.washingtonpost.com/opinions/2022/11/15/data-trump-weighed-down-republican-candidates/

